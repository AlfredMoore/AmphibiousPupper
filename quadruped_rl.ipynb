{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_evaluation(quaternion):\n",
    "    # TODO: read article and calculate sability\n",
    "    pass\n",
    "\n",
    "def speed_evaluation(quaternion):\n",
    "    # TODO: calculate speed\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class QuadrupEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    # metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    # TODO: Modify MAX_RATE\n",
    "    ABDUCTION_JOINT_MAX_RATE = 5\n",
    "    INNER_JOINT_MAX_RATE = 10\n",
    "    OUTER_JOINT_MAX_RATE = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super(QuadrupEnv, self).__init__()\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        \n",
    "        self.ABDUCTION_JOINT_MAX_RATE = 5\n",
    "        self.INNER_JOINT_MAX_RATE = 10\n",
    "        self.OUTER_JOINT_MAX_RATE = 10\n",
    "        self.QUATERNION_BOUND = 1.0\n",
    "        \n",
    "        self.TRADEOFF_PARAM = 0.5\n",
    "        \n",
    "        # Action Space Params\n",
    "        action_shape_3 = (3,)\n",
    "        action_low_3 = np.array([-1.0,-1.0,-1.0])\n",
    "        action_high_3 = np.array([1.0,1.0,1.0])\n",
    "        \n",
    "        action_shape_12 = (3,4)\n",
    "        action_low_12 = np.array([[-1.0,-1.0,-1.0,-1.0],\n",
    "                                  [-1.0,-1.0,-1.0,-1.0],\n",
    "                                  [-1.0,-1.0,-1.0,-1.0]])\n",
    "        \n",
    "        action_high_12 = np.array([[1.0, 1.0, 1.0, 1.0],\n",
    "                                   [1.0, 1.0, 1.0, 1.0],\n",
    "                                   [1.0, 1.0, 1.0, 1.0]])\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=action_low_12, high=action_high_12, \n",
    "            shape=action_shape_12, dtype=np.float32\n",
    "            )\n",
    "        \n",
    "        # The observation will be the coordisabilitynate of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        \n",
    "        # Observation Space Param -- data from IMU and sensors(optional)\n",
    "        obs_shape = (4,)    # quaternions\n",
    "        obs_low_3 = np.array([-1.0,-1.0,-1.0,-1.0])\n",
    "        obs_high_3 = np.array([1.0, 1.0, 1.0, 1.0])\n",
    "        \n",
    "        QUATERNION_BOUND = 1.0\n",
    "        obs_low_12 = np.array([[-1.0,-1.0,-1.0,-1.0],\n",
    "                               [-1.0,-1.0,-1.0,-1.0],\n",
    "                               [-1.0,-1.0,-1.0,-1.0]])\n",
    "        \n",
    "        obs_high_12 = np.array([[1.0, 1.0, 1.0, 1.0],\n",
    "                                [1.0, 1.0, 1.0, 1.0],\n",
    "                                [1.0, 1.0, 1.0, 1.0]])\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=obs_low_12, high=obs_high_12, shape=obs_shape, dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # initialize servo degree\n",
    "        self.servo_degree = np.array([[  0,  0,  0,  0],\n",
    "                                      [-45,-45,-45,-45],\n",
    "                                      [ 45, 45, 45, 45]])\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        # TODO: send msg that set the servo to [0, -45, 45]\n",
    "        # initialize the env and the policy\n",
    "        \n",
    "        \n",
    "        # initialize servo degree\n",
    "        self.servo_degree = np.array([[  0,  0,  0,  0],\n",
    "                                      [-45,-45,-45,-45],\n",
    "                                      [ 45, 45, 45, 45]])\n",
    "        \n",
    "        return np.array([0,0,0,0]).astype(np.float32), {}  # quaternion, empty info dict\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: difference of joint servo degree\n",
    "        [[x, x, x, x],[x, x, x, x],[x, x, x, x]]\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: what is the cornor case of terminated,\n",
    "        # too large IMU difference, drift from the supposed line?\n",
    "        terminated = False\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # TODO: reward = trade-off between speed and stablity\n",
    "        # 1. Receive socket msg\n",
    "        # 2. Calculate reward function\n",
    "        speed_reward = speed_evaluation()\n",
    "        stability_reward = stability_evaluation()\n",
    "        reward = self.TRADEOFF_PARAM * speed_reward + (1-self.TRADEOFF_PARAM) * stability_reward\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        print()\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
